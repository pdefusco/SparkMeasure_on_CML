{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkMeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sparkmeasure in ./.local/lib/python3.6/site-packages (0.14.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.6 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install sparkmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"PythonSQL\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.s3guard.ddb.region\",\"us-east-1\")\\\n",
    "    .config(\"spark.yarn.access.hadoopFileSystems\",\"s3a://demo-aws-1/\")\\\n",
    "    .config(\"spark.hadoop.yarn.resourcemanager.principal\",os.getenv(\"HADOOP_USER_NAME\"))\\\n",
    "    .config(\"spark.executor.instances\", 4)\\\n",
    "    .config(\"spark.executor.cores\", 4)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session\n",
    "# This example uses a local cluster, you can modify master to use  YARN or K8S if available \n",
    "# This example downloads sparkMeasure 0.14 for scala 2_11 from maven central\n",
    "\n",
    "spark = SparkSession \\\n",
    " .builder \\\n",
    " .master(\"local[*]\") \\\n",
    " .appName(\"Test sparkmeasure instrumentation of Python/PySpark code\") \\\n",
    " .config(\"spark.jars.packages\",\"ch.cern.sparkmeasure:spark-measure_2.11:0.14\")  \\\n",
    " .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.85.141:20049\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0.7.1.0.0-714</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Test sparkmeasure instrumentation of Python/PySpark code</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f02d1c64a20>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "| id|    Greeting|\n",
      "+---+------------+\n",
      "|  1|Hello world!|\n",
      "+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test that Spark is working OK\n",
    "spark.sql(\"select 1 as id, 'Hello world!' as Greeting\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    " .builder \\\n",
    " .master(\"local[*]\") \\\n",
    " .appName(\"Test sparkmeasure instrumentation of Python/PySpark code\") \\\n",
    " .config(\"spark.jars\",\"spark-measure_2.11-0.18-SNAPSHOT.jar\")  \\\n",
    " .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "| id|    Greeting|\n",
      "+---+------------+\n",
      "|  1|Hello world!|\n",
      "+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select 1 as id, 'Hello world!' as Greeting\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Python API in sparkmeasure package\n",
    "# an attache the sparkMeasure Listener for stagemetrics to the active Spark session\n",
    "\n",
    "from sparkmeasure import StageMetrics\n",
    "stagemetrics = StageMetrics(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cell and line magic to wrap the instrumentation\n",
    "from IPython.core.magic import (register_line_magic, register_cell_magic, register_line_cell_magic)\n",
    "\n",
    "@register_line_cell_magic\n",
    "def sparkmeasure(line, cell=None):\n",
    "    \"run and measure spark workload. Use: %sparkmeasure or %%sparkmeasure\"\n",
    "    val = cell if cell is not None else line\n",
    "    stagemetrics.begin()\n",
    "    eval(val)\n",
    "    stagemetrics.end()\n",
    "    stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| count(1)|\n",
      "+---------+\n",
      "|100000000|\n",
      "+---------+\n",
      "\n",
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 2\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 4\n",
      "sum(numTasks) => 7\n",
      "elapsedTime => 3141 (3 s)\n",
      "sum(stageDuration) => 3088 (3 s)\n",
      "sum(executorRunTime) => 5998 (6 s)\n",
      "sum(executorCpuTime) => 5960 (6 s)\n",
      "sum(executorDeserializeTime) => 18 (18 ms)\n",
      "sum(executorDeserializeCpuTime) => 17 (17 ms)\n",
      "sum(resultSerializationTime) => 6 (6 ms)\n",
      "sum(jvmGCTime) => 0 (0 ms)\n",
      "sum(shuffleFetchWaitTime) => 0 (0 ms)\n",
      "sum(shuffleWriteTime) => 10 (10 ms)\n",
      "max(resultSize) => 5340 (5.0 KB)\n",
      "sum(numUpdatedBlockStatuses) => 0\n",
      "sum(diskBytesSpilled) => 0 (0 Bytes)\n",
      "sum(memoryBytesSpilled) => 0 (0 Bytes)\n",
      "max(peakExecutionMemory) => 0\n",
      "sum(recordsRead) => 2100\n",
      "sum(bytesRead) => 0 (0 Bytes)\n",
      "sum(recordsWritten) => 0\n",
      "sum(bytesWritten) => 0 (0 Bytes)\n",
      "sum(shuffleTotalBytesRead) => 150 (150 Bytes)\n",
      "sum(shuffleTotalBlocksFetched) => 2\n",
      "sum(shuffleLocalBlocksFetched) => 2\n",
      "sum(shuffleRemoteBlocksFetched) => 0\n",
      "sum(shuffleBytesWritten) => 150 (150 Bytes)\n",
      "sum(shuffleRecordsWritten) => 2\n"
     ]
    }
   ],
   "source": [
    "%%sparkmeasure\n",
    "spark.sql(\"select count(*) from range(1000) cross join range(1000) cross join range(100)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregated Spark accumulables of type internal.metric. Sum of values grouped by metric name\n",
      "Name => sum(value) [group by name]\n",
      "\n",
      "executorCpuTime => 5961 (6 s)\n",
      "executorDeserializeCpuTime => 18 (18 ms)\n",
      "executorDeserializeTime => 18 (18 ms)\n",
      "executorRunTime => 5998 (6 s)\n",
      "input.recordsRead => 2100\n",
      "resultSerializationTime => 6 (6 ms)\n",
      "resultSize => 12438 (12.0 KB)\n",
      "shuffle.read.fetchWaitTime => 0 (0 ms)\n",
      "shuffle.read.localBlocksFetched => 2\n",
      "shuffle.read.localBytesRead => 150 (150 Bytes)\n",
      "shuffle.read.recordsRead => 2\n",
      "shuffle.read.remoteBlocksFetched => 0\n",
      "shuffle.read.remoteBytesRead => 0 (0 Bytes)\n",
      "shuffle.read.remoteBytesReadToDisk => 0 (0 Bytes)\n",
      "shuffle.write.bytesWritten => 150 (150 Bytes)\n",
      "shuffle.write.recordsWritten => 2\n",
      "shuffle.write.writeTime => 10 (10 ms)\n",
      "\n",
      "SQL Metrics and other non-internal metrics. Values grouped per accumulatorId and metric name.\n",
      "Accid, Name => max(value) [group by accId, name]\n",
      "\n",
      "   57, data size total => 29 (29 Bytes)\n",
      "   58, duration total => 4 (4 ms)\n",
      "   59, number of output rows => 1\n",
      "   62, aggregate time total => 4 (4 ms)\n",
      "   64, duration total => 5907 (6 s)\n",
      "   65, number of output rows => 2\n",
      "   68, aggregate time total => 5871 (6 s)\n",
      "   70, number of output rows => 100000000\n",
      "   71, number of output rows => 1000000\n",
      "   72, duration total => 5900 (6 s)\n",
      "   73, number of output rows => 1000\n",
      "   78, duration total => -1 (-1 ms)\n",
      "   79, number of output rows => 1000\n",
      "   85, number of output rows => 100\n"
     ]
    }
   ],
   "source": [
    "# Print additional metrics from accumulables\n",
    "stagemetrics.print_accumulables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| count(1)|\n",
      "+---------+\n",
      "|100000000|\n",
      "+---------+\n",
      "\n",
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 2\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 4\n",
      "sum(numTasks) => 7\n",
      "elapsedTime => 3588 (4 s)\n",
      "sum(stageDuration) => 3571 (4 s)\n",
      "sum(executorRunTime) => 7048 (7 s)\n",
      "sum(executorCpuTime) => 7039 (7 s)\n",
      "sum(executorDeserializeTime) => 4 (4 ms)\n",
      "sum(executorDeserializeCpuTime) => 3 (3 ms)\n",
      "sum(resultSerializationTime) => 1 (1 ms)\n",
      "sum(jvmGCTime) => 0 (0 ms)\n",
      "sum(shuffleFetchWaitTime) => 0 (0 ms)\n",
      "sum(shuffleWriteTime) => 0 (0 ms)\n",
      "max(resultSize) => 5297 (5.0 KB)\n",
      "sum(numUpdatedBlockStatuses) => 0\n",
      "sum(diskBytesSpilled) => 0 (0 Bytes)\n",
      "sum(memoryBytesSpilled) => 0 (0 Bytes)\n",
      "max(peakExecutionMemory) => 0\n",
      "sum(recordsRead) => 2100\n",
      "sum(bytesRead) => 0 (0 Bytes)\n",
      "sum(recordsWritten) => 0\n",
      "sum(bytesWritten) => 0 (0 Bytes)\n",
      "sum(shuffleTotalBytesRead) => 150 (150 Bytes)\n",
      "sum(shuffleTotalBlocksFetched) => 2\n",
      "sum(shuffleLocalBlocksFetched) => 2\n",
      "sum(shuffleRemoteBlocksFetched) => 0\n",
      "sum(shuffleBytesWritten) => 150 (150 Bytes)\n",
      "sum(shuffleRecordsWritten) => 2\n"
     ]
    }
   ],
   "source": [
    "# You can also explicitly Wrap your Spark workload into stagemetrics instrumentation \n",
    "# as in this example\n",
    "stagemetrics.begin()\n",
    "\n",
    "spark.sql(\"select count(*) from range(1000) cross join range(1000) cross join range(100)\").show()\n",
    "\n",
    "stagemetrics.end()\n",
    "# Print a summary report\n",
    "stagemetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| count(1)|\n",
      "+---------+\n",
      "|100000000|\n",
      "+---------+\n",
      "\n",
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Context default degree of parallelism = 2\n",
      "Aggregated Spark stage metrics:\n",
      "numStages => 4\n",
      "sum(numTasks) => 7\n",
      "elapsedTime => 3562 (4 s)\n",
      "sum(stageDuration) => 3546 (4 s)\n",
      "sum(executorRunTime) => 6989 (7 s)\n",
      "sum(executorCpuTime) => 6982 (7 s)\n",
      "sum(executorDeserializeTime) => 7 (7 ms)\n",
      "sum(executorDeserializeCpuTime) => 4 (4 ms)\n",
      "sum(resultSerializationTime) => 0 (0 ms)\n",
      "sum(jvmGCTime) => 0 (0 ms)\n",
      "sum(shuffleFetchWaitTime) => 0 (0 ms)\n",
      "sum(shuffleWriteTime) => 0 (0 ms)\n",
      "max(resultSize) => 5254 (5.0 KB)\n",
      "sum(numUpdatedBlockStatuses) => 0\n",
      "sum(diskBytesSpilled) => 0 (0 Bytes)\n",
      "sum(memoryBytesSpilled) => 0 (0 Bytes)\n",
      "max(peakExecutionMemory) => 0\n",
      "sum(recordsRead) => 2100\n",
      "sum(bytesRead) => 0 (0 Bytes)\n",
      "sum(recordsWritten) => 0\n",
      "sum(bytesWritten) => 0 (0 Bytes)\n",
      "sum(shuffleTotalBytesRead) => 150 (150 Bytes)\n",
      "sum(shuffleTotalBlocksFetched) => 2\n",
      "sum(shuffleLocalBlocksFetched) => 2\n",
      "sum(shuffleRemoteBlocksFetched) => 0\n",
      "sum(shuffleBytesWritten) => 150 (150 Bytes)\n",
      "sum(shuffleRecordsWritten) => 2\n"
     ]
    }
   ],
   "source": [
    "# Another way to encapsulate code and instrumentation in a compact form\n",
    "\n",
    "stagemetrics.runandmeasure(locals(), \"\"\"\n",
    "spark.sql(\"select count(*) from range(1000) cross join range(1000) cross join range(100)\").show()\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| count(1)|\n",
      "+---------+\n",
      "|100000000|\n",
      "+---------+\n",
      "\n",
      "\n",
      "Scheduling mode = FIFO\n",
      "Spark Contex default degree of parallelism = 2\n",
      "Aggregated Spark task metrics:\n",
      "numtasks => 7\n",
      "elapsedTime => 4594 (5 s)\n",
      "sum(duration) => 9116 (9 s)\n",
      "sum(schedulerDelay) => 3\n",
      "sum(executorRunTime) => 9110 (9 s)\n",
      "sum(executorCpuTime) => 9080 (9 s)\n",
      "sum(executorDeserializeTime) => 3 (3 ms)\n",
      "sum(executorDeserializeCpuTime) => 1 (1 ms)\n",
      "sum(resultSerializationTime) => 0 (0 ms)\n",
      "sum(jvmGCTime) => 18 (18 ms)\n",
      "sum(shuffleFetchWaitTime) => 0 (0 ms)\n",
      "sum(shuffleWriteTime) => 0 (0 ms)\n",
      "sum(gettingResultTime) => 0 (0 ms)\n",
      "max(resultSize) => 2627 (2.0 KB)\n",
      "sum(numUpdatedBlockStatuses) => 0\n",
      "sum(diskBytesSpilled) => 0 (0 Bytes)\n",
      "sum(memoryBytesSpilled) => 0 (0 Bytes)\n",
      "max(peakExecutionMemory) => 0\n",
      "sum(recordsRead) => 2100\n",
      "sum(bytesRead) => 0 (0 Bytes)\n",
      "sum(recordsWritten) => 0\n",
      "sum(bytesWritten) => 0 (0 Bytes)\n",
      "sum(shuffleTotalBytesRead) => 150 (150 Bytes)\n",
      "sum(shuffleTotalBlocksFetched) => 2\n",
      "sum(shuffleLocalBlocksFetched) => 2\n",
      "sum(shuffleRemoteBlocksFetched) => 0\n",
      "sum(shuffleBytesWritten) => 150 (150 Bytes)\n",
      "sum(shuffleRecordsWritten) => 2\n"
     ]
    }
   ],
   "source": [
    "from sparkmeasure import TaskMetrics\n",
    "taskmetrics = TaskMetrics(spark)\n",
    "\n",
    "taskmetrics.begin()\n",
    "spark.sql(\"select count(*) from range(1000) cross join range(1000) cross join range(100)\").show()\n",
    "taskmetrics.end()\n",
    "taskmetrics.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------+-----+-------------+-------------+--------+--------------+----------+---------+------------+-----------+-----------------+----------+---------------+---------------+-----------------------+--------------------------+-----------------------+---------+----------+-----------------------+----------------+------------------+-------------------+-----------+---------+--------------+------------+--------------------+---------------------+-------------------------+-------------------------+--------------------------+----------------+-------------------+---------------------+\n",
      "|jobId|jobGroup|stageId|index|   launchTime|   finishTime|duration|schedulerDelay|executorId|     host|taskLocality|speculative|gettingResultTime|successful|executorRunTime|executorCpuTime|executorDeserializeTime|executorDeserializeCpuTime|resultSerializationTime|jvmGCTime|resultSize|numUpdatedBlockStatuses|diskBytesSpilled|memoryBytesSpilled|peakExecutionMemory|recordsRead|bytesRead|recordsWritten|bytesWritten|shuffleFetchWaitTime|shuffleTotalBytesRead|shuffleTotalBlocksFetched|shuffleLocalBlocksFetched|shuffleRemoteBlocksFetched|shuffleWriteTime|shuffleBytesWritten|shuffleRecordsWritten|\n",
      "+-----+--------+-------+-----+-------------+-------------+--------+--------------+----------+---------+------------+-----------+-----------------+----------+---------------+---------------+-----------------------+--------------------------+-----------------------+---------+----------+-----------------------+----------------+------------------+-------------------+-----------+---------+--------------+------------+--------------------+---------------------+-------------------------+-------------------------+--------------------------+----------------+-------------------+---------------------+\n",
      "|   17|    null|     29|  867|1599687907690|1599687907692|       2|             0|    driver|localhost|           0|      false|                0|      true|              1|              1|                      1|                         0|                      0|        0|      1332|                      0|               0|                 0|                  0|        500|        0|             0|           0|                   0|                    0|                        0|                        0|                         0|               0|                  0|                    0|\n",
      "|   17|    null|     29|  866|1599687907690|1599687907693|       3|             1|    driver|localhost|           0|      false|                0|      true|              1|              1|                      1|                         0|                      0|        0|      1332|                      0|               0|                 0|                  0|        500|        0|             0|           0|                   0|                    0|                        0|                        0|                         0|               0|                  0|                    0|\n",
      "|   18|    null|     30|  869|1599687907693|1599687907694|       1|             0|    driver|localhost|           0|      false|                0|      true|              1|              0|                      0|                         0|                      0|        0|      1282|                      0|               0|                 0|                  0|         50|        0|             0|           0|                   0|                    0|                        0|                        0|                         0|               0|                  0|                    0|\n",
      "|   18|    null|     30|  868|1599687907693|1599687907695|       2|             1|    driver|localhost|           0|      false|                0|      true|              1|              0|                      0|                         0|                      0|        0|      1282|                      0|               0|                 0|                  0|         50|        0|             0|           0|                   0|                    0|                        0|                        0|                         0|               0|                  0|                    0|\n",
      "|   19|    null|     31|  871|1599687907718|1599687912264|    4546|             0|    driver|localhost|           0|      false|                0|      true|           4546|           4532|                      0|                         1|                      0|        9|      2627|                      0|               0|                 0|                  0|        500|        0|             0|           0|                   0|                    0|                        0|                        0|                         0|               0|                 75|                    1|\n",
      "|   19|    null|     31|  870|1599687907718|1599687912277|    4559|             1|    driver|localhost|           0|      false|                0|      true|           4558|           4545|                      0|                         0|                      0|        9|      2627|                      0|               0|                 0|                  0|        500|        0|             0|           0|                   0|                    0|                        0|                        0|                         0|               0|                 75|                    1|\n",
      "|   19|    null|     32|  872|1599687912281|1599687912284|       3|             0|    driver|localhost|           4|      false|                0|      true|              2|              1|                      1|                         0|                      0|        0|      1612|                      0|               0|                 0|                  0|          0|        0|             0|           0|                   0|                  150|                        2|                        2|                         0|               0|                  0|                    0|\n",
      "+-----+--------+-------+-----+-------------+-------------+--------+--------------+----------+---------+------------+-----------+-----------------+----------+---------------+---------------+-----------------------+--------------------------+-----------------------+---------+----------+-----------------------+----------------+------------------+-------------------+-----------+---------+--------------+------------+--------------------+---------------------+-------------------------+-------------------------+--------------------------+----------------+-------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from PerfTaskMetrics\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
